\documentclass[12pt]{book}

\title{Linear Algebra II}
\author{Fall Semester\\MATH201}
\date{}

\usepackage{indentfirst}
\usepackage{kotex}
\usepackage{amsmath, amssymb, amsthm, amsfonts, graphics, epsfig, fancyhdr, bm, hyperref, tocloft, etoc, titletoc, thmtools}
\usepackage{tikz-cd}
\usepackage[shortlabels]{enumitem}
\setlength{\headheight}{28pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Diagonalizability}
\fancyfoot[C]{\thepage}

\setlength\parindent{12pt}
\theoremstyle{definition}

\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remarks}
\newtheorem{problem}{Problem}
\newtheorem*{definition}{Definition}
\setcounter{tocdepth}{3}

\usepackage{chngcntr}
\makeatletter
% section from book
%\newcommand\section{\@startsection {section}{1}{\z@}%
	%                                   {-3.5ex \@plus -1ex \@minus -.2ex}%
	%                                   {2.3ex \@plus.2ex}%
	%                                   {\normalfont\Large\bfseries}}
\renewcommand\chapter{\@startsection {chapter}{0}{\z@}%
	{-4.5ex \@plus -1ex \@minus -.2ex}%
	{3.3ex \@plus.2ex}%
	{\normalfont\LARGE\bfseries}}
\makeatletter

\counterwithin{problem}{chapter}
\renewcommand\thechapter{5.\arabic{chapter}}
\renewcommand\thetheorem{5.\arabic{theorem}}
\renewcommand{\theequation}{\arabic{equation}}
\renewcommand{\qedsymbol}{\ensuremath{}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\setlength{\cftchapindent}{2em}
\setlength{\cftsecindent}{4em}
\setlength{\cftsubsecindent}{6em}
\setlength{\cftchapnumwidth}{2em}
\setlength{\cftsecnumwidth}{3em}
\setlength{\cftsubsecnumwidth}{4em}

\begin{document}
	\setcounter{chapter}{1}
	\listoftheorems[title = Contents]
	\newpage
	\chapter{Diagonalizability}
	\setcounter{theorem}{4}
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$, and let $\lambda_{1}$, $\lambda_{2}$, $\ldots$, $\lambda_k$ be distinct eigenvalues of $\mathsf{T}$. If $v_1,~v_2,~\ldots,~v_k$ are eigenvectors of $\mathsf{T}$ such that $\lambda_i$ corresponds to $v_i\ (1\leq i\leq k)$, then $\left\{v_1,~v_2,~\ldots,~v_k\right\}$ is linearly independent.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\vfill
	\begin{corollary*}
		Let $\mathsf{T}$ be a linear operator on an $n-$dimensional vector space $\mathsf{V}$. If $\mathsf{T}$ has $n$ distinct eigenvalues, then $\mathsf{T}$ is diagonalizable.
	\end{corollary*}
	\begin{proof}
	\end{proof}
	\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip\bigskip
	\newpage
	\begin{definition}
		A polynomial $f(t)$ in $\mathsf{P}(F)$ \textbf{splits over} $F$ if there are scalars $c$, $a_1,~a_2,~\ldots,~a_n$ (not necessarily distinct) in $F$ such that
		$$f(t)=c(t-a_1)(t-a_2)\cdots(t-a_n).$$
	\end{definition}
	\vspace*{\fill}
	\begin{definition}
		Let $\lambda$ be an eigenvalue of a linear operator or matrix with characteristic polynomial $f(t)$. The \textbf{(algebraic) multiplicity} of $\lambda$ is the largest positive integer $k$ for which $(t-\lambda)^k$ is a factor of $f(t)$.
	\end{definition}
	\vspace*{\fill}
	\begin{definition}
		Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$, and let $\lambda$ be an eigenvalue of $\mathsf{T}$. Define $\mathsf{E}_\lambda=\left\{x\in \mathsf{V}\ :\ \mathsf{T}(x)=\lambda x\right\}=\mathsf{N}(\mathsf{T}-\lambda\mathsf{I}_\mathsf{V})$ The Set $\mathsf{E}_\lambda$ is called the \textbf{eigenspace} of $\mathsf{T}$ corresponding to the eigenvalue $\lambda$. Analogously, we define the \textbf{eigenspace} of a square matrix $A$ to be the eigenspace of $\mathsf{L}_A$.
	\end{definition}
	\vspace*{\fill}
	\newpage
	\begin{theorem}
		The characteristic polynomial of any diagonalizable linear operator splits.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\lambda$ be an eigenvalue of $\mathsf{T}$ having multiplicity $m$. Then $1\leq \dim(\mathsf{E}_\lambda)\leq m$.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{lemma*}
		Let $\mathsf{T}$ be a linear operator, and let $\lambda_{1}$, $\lambda_2$, $\ldots$, $\lambda_k$ be distinct eigenvalues of $\mathsf{T}$. For each $i=1,~2,~\ldots,~k$, let $v_i\in\mathsf{E}_{\lambda_i}$, the eigenspace corresponding to $\lambda_i$. If $$v_1+v_2+\cdots+v_k=\mathit{0},$$
		then $v_i=\mathit{0}$ for all $i$.
	\end{lemma*}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$, and let $\lambda_1$, $\lambda_2$, $\ldots$, $\lambda_k$ be distinct eigenvalues of $\mathsf{T}$. For each $i=1,~2,~\ldots,~k$, let $S_i$ be a finite linearly independent subset of the eigenspace $\mathsf{E}_{\lambda_i}$. Then $S=S_1\cup S_2\cup \cdots\cup S_k$ is a linearly independent subset of $\mathsf{V}$.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{theorem} Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$ such that the characteristic polynomial of $\mathsf{T}$ splits. Let $\lambda_1$, $\lambda_2$, $\ldots$, $\lambda_k$ be the distinct eigenvalues of $\mathsf{T}$. Then
		\begin{enumerate}[(a)]
			\item $\mathsf{T}$ is diagonalizable if and only if the multiplicity of $\lambda_i$ is equal to $\dim(\mathsf{E}_{\lambda_i})$ for all $i$.
			\item if $\mathsf{T}$ is diagonalizable and $\beta_i$ is an ordered basis for $\mathsf{E}_{\lambda_i}$ for each $i$, then $\beta=\beta_1\cup \beta_2\cup\cdots\cup\beta_k$ is an ordered basis for $\mathsf{V}$ consisting of eigenvectors of $\mathsf{T}$.
			\item State and prove results analogous to (a) and (b) for matrices.
		\end{enumerate}
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{corollary*}
		Let $\mathsf{T}$ be a linear operator on an $n-$dimensionala vector space $\mathsf{V}$. Then $\mathsf{T}$ is diagonalizable if and only if both of the following conditions hold.
		\begin{enumerate}
			\item The characteristic polynomial of $\mathsf{T}$ splits.
			\item For each eigenvalue $\lambda$ of $\mathsf{T}$, the multiplicity of $\lambda$ equals $n-\text{rank}(\mathsf{T}-\lambda \mathsf{I})$.
		\end{enumerate}
	\end{corollary*}
	\newpage
	\section*{Direct sums}
	\begin{definition}
		Let $\mathsf{W}_1$, $\mathsf{W}_2$, $\ldots$, $\mathsf{W}_k$ be subspaces of a vector space $\mathsf{V}$. We define the \textbf{sum} of these subspaces to be the set $$\left\{v_1+v_2+\cdots+v_k\,:\,v_i\in\mathsf{W}_i~\text{for }1\leq i\leq k\right\},$$
		which we denote by $\mathsf{W}_1+\mathsf{W}_2+\cdots+\mathsf{W}_k$ or $\displaystyle\sum_{i=1}^k \mathsf{W}_i$.
	\end{definition}
	\vspace*{\fill}
	\begin{definition}
		Let $\mathsf{W}_1$, $\mathsf{W}_2$, $\ldots$, $\mathsf{W}_k$ be subspaces of a vector space $\mathsf{V}$. We call $\mathsf{V}$ the \textbf{direct sum} of the subspaces $\mathsf{W}_1$, $\mathsf{W}_2$, $\ldots$, $\mathsf{W}_k$ and write $\mathsf{V}=\mathsf{W}_1\oplus\mathsf{W}_2\oplus\cdots\oplus\mathsf{W}_k$, If$$\mathsf{V}=\displaystyle\sum_{i=1}^k\mathsf{W}_i$$ and $$\mathsf{W}_j\cap\displaystyle\sum_{i\neq j}\mathsf{W}_i=\left\{0\right\}\quad\text{for each }j\ (1\leq j\leq k).$$
	\end{definition}
	\vspace*{\fill}
	\vspace*{\fill}
	\newpage
	\begin{theorem}
		Let $\mathsf{W}_1$, $\mathsf{W}_2$, $\ldots$, $\mathsf{W}_k$ be subspaces of a finite-dimensional vector space $\mathsf{V}$. The following conditions are equivalent.
		\begin{enumerate}[(a)]
			\item $\mathsf{V}=\mathsf{W}_1\oplus\mathsf{W}_2\oplus\cdots\oplus\mathsf{W}_k$
			\item $\mathsf{V}=\displaystyle\sum_{i=1}^k \mathsf{W}_i$ and, for any vectors $v_1$, $v_2$, $\ldots$, $v_k$ such that $v_i\in\mathsf{W}_i\ (1\leq i\leq k)$, if $v_1+v_2+\cdots+v_k=\mathit{0}$, then $v_i=\mathit{0}$ for all $i$.
			\item Each vector $v\in\mathsf{V}$ can be uniquely written as $v=v_1+v_2+\cdots+v_k$, where $v_i\in\mathsf{W}_i$.
			\item If $\gamma_i$ is an ordered basis for $\mathsf{W}_i\ (1\leq i\leq k)$, then $\gamma_1\cup\gamma_2\cup\cdots\cup\gamma_k$ is an ordered basis for $\mathsf{V}$.
			\item For each $i=1$, $2$, $\ldots$, $k$, there exists an ordered basis $\gamma_i$ for $\mathsf{W}_i$ such that $\gamma_1\cup\gamma_2\cup\cdots\cup\gamma_k$ is an ordered basis for $\mathsf{V}$.
		\end{enumerate}
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{proof}{Continued...}
		
	\end{proof}
	\newpage
	\begin{theorem}
		A linear operator $\mathsf{T}$ on a finite-dimensional vector space $\mathsf{V}$ is diagonalizable if and only if $\mathsf{V}$ is the direct sum of the eigenspaces of $\mathsf{T}$.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\setcounter{chapter}{5}
	\newpage
	\chapter{Invariant subspaces and the Cayleyâ€“Hamilton Theorem}
	\setcounter{theorem}{20}
	\begin{definition}
		Let $\mathsf{T}$ be a linear operator on a vector space $\mathsf{V}$. A subspace $\mathsf{W}$ of $\mathsf{V}$ is called a $\mathsf{T}$\textbf{-invariant subspace} of $\mathsf{V}$ if $\mathsf{T}(\mathsf{W})\subseteq \mathsf{W}$, that is, if $\mathsf{T}(v)\in\mathsf{W}$ for all $v\in\mathsf{W}$.
	\end{definition}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\mathsf{W}$ be a $\mathsf{T}$-invariant subspace of $\mathsf{V}$. Then the characteristic polynomial of $\mathsf{T}_\mathsf{W}$ divides the characteristic polynomial of $\mathsf{T}$.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\mathsf{W}$ denote the $\mathsf{T}$-cyclic subspace of $\mathsf{V}$ generated by a nonzero vector $v\in\mathsf{V}$. Let $k=\dim(\mathsf{W})$. Then
		\begin{enumerate}[(a)]
			\item $\left\{v,~\mathsf{T}(v),~\mathsf{T}^2(v),\ldots,~\mathsf{T}^{k-1}(v)\right\}$ is a basis for $\mathsf{W}$.
			\item If $a_0v+a_1\mathsf{T}(v)+\cdots+a_{k-1}\mathsf{T}^{k-1}(v)+\mathsf{T}^k(v)=\mathit{0},$ then the characteristic polynomial of $\mathsf{T}_\mathsf{W}$ is $f(t)=(-1)^k(a_0+a_1t+\cdots+a_{k-1}t^{k-1}+t^k).$
		\end{enumerate}
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\subsection*{The Cayley-Hamilton Theorem}
	\begin{theorem}[\textbf{Cayley-Hamilton}]
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $f(t)$ be the characteristic polynomial of $\mathsf{T}$. Then $f(\mathsf{T})=\mathsf{T}_0$, the zero transformation. That is, $\mathsf{T}$ "satisfies" its characteristic equation.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\vspace*{\fill}
	\begin{corollary*}[\textbf{Cayley-Hamilton Theorem for Matrices}]
		Let $\mathsf{A}$ be an $n\times n$ matrix, and let $f(t)$ be the characteristic polynomial of $A$. Then $f(A)=O$, the $n\times n$ zero matrix.
	\end{corollary*}
	\vspace*{\fill}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and suppose that $\mathsf{V}=\mathsf{W}_1\oplus\mathsf{W}_2\oplus\cdots\oplus\mathsf{W}_k$, where $\mathsf{W}_i$ is a $\mathsf{T}$-invariant subspace of $\mathsf{V}$ for each $i\ (1\leq i \leq k).$ Suppose that $f_i(t)$ is the characteristic polynomial of $\mathsf{T}_{\mathsf{W}_i}\ (1\leq i \leq k).$ Then $f_1(t)\cdot f_2(t)\cdot \cdots \cdot f_k(t)$ is the characteristic polynomial of $\mathsf{T}$.
	\end{theorem}
	\begin{proof}
	\end{proof}
	\newpage
	\begin{definition}
		Let $B_1\in\mathsf{M}_{m \times m}(F)$, and let $B_2\in\mathsf{M}_{n\times n}(F)$. We define the \textbf{direct sum} of $B_1$ and $B_2$, denoted $B_1\oplus B_2$, as the $(m+n)\times(m+n)$ matrix $A$ such that
		$$A_{ij}=\begin{cases}
			(B_1)_{ij}&\text{for }1\leq i,\ j\leq m\\
			(B_2)_{(i-m),\ (j-m)}&\text{for }m+1\leq i,\ j\leq n+m\\
			0&\text{otherwise}
		\end{cases}$$
		If $B_1$, $B_2$, $\ldots$, $B_k$ are square matrices with entries from $F$, then we define the \textbf{direct sum} of $B_1$, $B_2$, $\ldots$, $B_k$ recursively by $$B_1\oplus B_2\oplus \cdots \oplus B_k=(B_1\oplus B_2\oplus\cdots\oplus B_{k-1})\oplus B_k.$$
		If $A=B_1\oplus B_2\oplus \cdots \oplus B_k$, then we often write
		$$A=\begin{pmatrix}
			B_1&O&\cdots&O\\
			O&B_2&\cdots&O\\
			\vdots&\vdots&&\vdots\\
			O&O&\cdots&B_k
		\end{pmatrix}.$$
	\end{definition}
	\newpage
	\begin{theorem}
		Let $\mathsf{T}$ be a linear operator on a finite-dimensional vector space $\mathsf{V}$, and let $\mathsf{W}_1$, $\mathsf{W}_2$, $\ldots$, $\mathsf{W}_k$ be $\mathsf{T}$-invariant subspaces of $\mathsf{V}$ such that $\mathsf{V}=\mathsf{W}_1\oplus\mathsf{W}_2\oplus\cdots\oplus\mathsf{W}_k.$ For each $i$, let $\beta_i$ be an ordered basis for $\mathsf{W}_i$, and let $\beta=\beta_1\cup\beta_2\cup\cdots\cup\beta_k$. Let $A=[\mathsf{T}]_\beta$ and $B_i=[\mathsf{T}_{\mathsf{W}_i}]_{\beta_i}$ for $i=1,~2,~\ldots,~k$. Then $A=B_1\oplus B_2\oplus \cdots \oplus B_k.$
	\end{theorem}
	\begin{proof}
	\end{proof}
\end{document}